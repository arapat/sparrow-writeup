
@inproceedings{domingo_scaling_2000,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Scaling {Up} a {Boosting}-{Based} {Learner} via {Adaptive} {Sampling}},
	isbn = {978-3-540-67382-8 978-3-540-45571-4},
	url = {https://link.springer.com/chapter/10.1007/3-540-45571-X_37},
	doi = {10.1007/3-540-45571-X_37},
	abstract = {In this paper we present a experimental evaluation of a boosting based learning system and show that can be run efficiently over a large dataset. The system uses as base learner decision stumps, single atribute decision trees with only two terminal nodes. To select the best decision stump at each iteration we use an adaptive sampling method. As a boosting algorithm, we use a modification of AdaBoost that is suitable to be combined with a base learner that does not use all the dataset. We provide experimental evidence that our method is as accurate as the equivalent algorithm that uses all the dataset but much faster.},
	language = {en},
	urldate = {2018-04-26},
	booktitle = {Knowledge {Discovery} and {Data} {Mining}. {Current} {Issues} and {New} {Applications}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Domingo, Carlos and Watanabe, Osamu},
	month = apr,
	year = {2000},
	pages = {317--328},
	file = {Domingo_Watanabe_2000_Scaling_Up_a_Boosting-Based_Learner_via_Adaptive_Sampling.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Domingo_Watanabe_2000_Scaling_Up_a_Boosting-Based_Learner_via_Adaptive_Sampling.pdf:application/pdf}
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	number = {1},
	urldate = {2018-04-27},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
	file = {Freund_Schapire_1997_A_Decision-Theoretic_Generalization_of_On-Line_Learning_and_an_Application_to.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Freund_Schapire_1997_A_Decision-Theoretic_Generalization_of_On-Line_Learning_and_an_Application_to.pdf:application/pdf;ScienceDirect Snapshot:/Users/arapat/Documents/Geisel/storage/5P373W7Y/S002200009791504X.html:text/html}
}

@incollection{abuzaid_yggdrasil:_2016,
	title = {Yggdrasil: {An} {Optimized} {System} for {Training} {Deep} {Decision} {Trees} at {Scale}},
	shorttitle = {Yggdrasil},
	url = {http://papers.nips.cc/paper/6366-yggdrasil-an-optimized-system-for-training-deep-decision-trees-at-scale.pdf},
	urldate = {2018-04-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Abuzaid, Firas and Bradley, Joseph K and Liang, Feynman T and Feng, Andrew and Yang, Lee and Zaharia, Matei and Talwalkar, Ameet S},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {3817--3825},
	file = {Abuzaid_et_al_2016_Yggdrasil.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/synchronous/Abuzaid_et_al_2016_Yggdrasil.pdf:application/pdf;NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/SSH2CX9M/6366-yggdrasil-an-optimized-system-for-training-deep-decision-trees-at-scale.html:text/html}
}

@article{kitagawa_monte_1996,
	title = {Monte {Carlo} {Filter} and {Smoother} for {Non}-{Gaussian} {Nonlinear} {State} {Space} {Models}},
	volume = {5},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474692},
	doi = {10.1080/10618600.1996.10474692},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Kitagawa, Genshiro},
	year = {1996},
	pages = {1--25},
	file = {Kitagawa_1996_Monte_Carlo_Filter_and_Smoother_for_Non-Gaussian_Nonlinear_State_Space_Models.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/statistics/Kitagawa_1996_Monte_Carlo_Filter_and_Smoother_for_Non-Gaussian_Nonlinear_State_Space_Models.pdf:application/pdf}
}

@article{balsubramani_sharp_2014,
	title = {Sharp {Finite}-{Time} {Iterated}-{Logarithm} {Martingale} {Concentration}},
	url = {http://arxiv.org/abs/1405.2639},
	abstract = {We give concentration bounds for martingales that are uniform over finite times and extend classical Hoeffding and Bernstein inequalities. We also demonstrate our concentration bounds to be optimal with a matching anti-concentration inequality, proved using the same method. Together these constitute a finite-time version of the law of the iterated logarithm, and shed light on the relationship between it and the central limit theorem.},
	urldate = {2018-04-29},
	journal = {arXiv:1405.2639 [cs, math, stat]},
	author = {Balsubramani, Akshay},
	month = may,
	year = {2014},
	note = {arXiv: 1405.2639},
	keywords = {60E15, 60G17 (Primary), 60G40, 60G42, 60G44 (Secondary), Computer Science - Learning, Mathematics - Probability, Statistics - Machine Learning},
	annote = {Comment: 25 pages},
	file = {arXiv.org Snapshot:/Users/arapat/Documents/Geisel/storage/YPDIT4BQ/1405.html:text/html;Balsubramani_2014_Sharp_Finite-Time_Iterated-Logarithm_Martingale_Concentration.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/sequential-analysis/Balsubramani_2014_Sharp_Finite-Time_Iterated-Logarithm_Martingale_Concentration.pdf:application/pdf}
}

@inproceedings{sonnenburg_coffin:_2010,
	address = {USA},
	series = {{ICML}'10},
	title = {{COFFIN}: {A} {Computational} {Framework} for {Linear} {SVMs}},
	isbn = {978-1-60558-907-7},
	shorttitle = {{COFFIN}},
	url = {http://dl.acm.org/citation.cfm?id=3104322.3104449},
	abstract = {In a variety of applications, kernel machines such as Support Vector Machines (SVMs) have been used with great success often delivering state-of-the-art results. Using the kernel trick, they work on several domains and even enable heterogeneous data fusion by concatenating feature spaces or multiple kernel learning. Unfortunately, they are not suited for truly large-scale applications since they suffer from the curse of supporting vectors, i.e., the speed of applying SVMs decays linearly with the number of support vectors. In this paper we develop COFFIN — a new training strategy for linear SVMs that effectively allows the use of on demand computed kernel feature spaces and virtual examples in the primal. With linear training and prediction effort this framework leverages SVM applications to truly large-scale problems: As an example, we train SVMs for human splice site recognition involving 50 million examples and sophisticated string kernels. Additionally, we learn an SVM based gender detector on 5 million examples on low-tech hardware and achieve beyond the state-of-the-art accuracies on both tasks. Source code, data sets and scripts are freely available from http://sonnenburgs.de/soeren/coffin.},
	urldate = {2018-04-29},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Sonnenburg, Soeren and Franc, Vojtěch},
	year = {2010},
	pages = {999--1006},
	file = {Sonnenburg_Franc_2010_COFFIN.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/bioinformatics/Sonnenburg_Franc_2010_COFFIN.pdf:application/pdf}
}

@book{bekkerman_scaling_2012,
	title = {Scaling {Up} {Machine} {Learning}: {Parallel} and {Distributed} {Approaches}},
	isbn = {978-0-521-19224-8},
	shorttitle = {Scaling {Up} {Machine} {Learning}},
	abstract = {This book presents an integrated collection of representative approaches for scaling up machine learning and data mining methods on parallel and distributed computing platforms. Demand for parallelizing learning algorithms is highly task-specific: in some settings it is driven by the enormous dataset sizes, in others by model complexity or by real-time performance requirements. Making task-appropriate algorithm and platform choices for large-scale machine learning requires understanding the benefits, trade-offs, and constraints of the available options. Solutions presented in the book cover a range of parallelization platforms from FPGAs and GPUs to multi-core systems and commodity clusters, concurrent programming frameworks including CUDA, MPI, MapReduce, and DryadLINQ, and learning settings (supervised, unsupervised, semi-supervised, and online learning). Extensive coverage of parallelization of boosted trees, SVMs, spectral clustering, belief propagation and other popular learning algorithms and deep dives into several applications make the book equally useful for researchers, students, and practitioners.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Bekkerman, Ron and Bilenko, Mikhail and Langford, John},
	year = {2012},
	note = {Google-Books-ID: c5v5USMvcMYC},
	keywords = {Computers / Computer Vision \& Pattern Recognition, Computers / General, Computers / Intelligence (AI) \& Semantics, Computers / Optical Data Processing},
	file = {Bekkerman_et_al_2012_Scaling_Up_Machine_Learning.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/large-scale-learning/Bekkerman_et_al_2012_Scaling_Up_Machine_Learning.pdf:application/pdf}
}

@article{agarwal_reliable_2014,
	title = {A {Reliable} {Effective} {Terascale} {Linear} {Learning} {System}},
	volume = {15},
	url = {http://jmlr.org/papers/v15/agarwal14a.html},
	journal = {Journal of Machine Learning Research},
	author = {Agarwal, Alekh and Chapelle, Oliveier and Dudík, Miroslav and Langford, John},
	year = {2014},
	pages = {1111--1133},
	file = {Agarwal_et_al_2014_A_Reliable_Effective_Terascale_Linear_Learning_System.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/large-scale-learning/Agarwal_et_al_2014_A_Reliable_Effective_Terascale_Linear_Learning_System.pdf:application/pdf}
}

@book{wald_sequential_1973,
	title = {Sequential {Analysis}},
	isbn = {978-0-486-61579-0},
	abstract = {In 1943, while in charge of Columbia University's Statistical Research Group, Abraham Wald devised Sequential Design, an innovative statistical inference system. Because the decision to terminate an experiment is not predetermined, sequential analysis can arrive at a decision much sooner and with substantially fewer observations than equally reliable test procedures based on a predetermined number of observations. The system's immense value was immediately recognized, and its use was restricted to wartime research and procedures. In 1945, it was released to the public and has since revolutionized many aspects of statistical practice. This book is Professor Wald's own description of the system. Part I contains a discussion of the general theory of the sequential probability ratio test, with comparisons to traditional statistical inference systems. Part II discusses applications that illustrate the general theory and raise points of theoretical interest specific to these applications. Part III outlines a possible approach to the problem of sequential multi-valued decisions and estimation. All three sections can be understood by readers with only a background in college algebra and a first course in calculus. Mathematical derivations of somewhat intricate nature appear in the appendix. Sequential Analysis offers statistical researchers a time- and money-saving approach, introduces students to one of the major systems in contemporary use, and presents those already acquainted with the system with valuable background information.},
	language = {en},
	publisher = {Courier Corporation},
	author = {Wald, Abraham},
	year = {1973},
	note = {Google-Books-ID: zXqPAQAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@inproceedings{chen_xgboost:_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {http://doi.acm.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2018-04-29},
	booktitle = {Proceedings of the 22Nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	keywords = {large-scale, learning, machine},
	pages = {785--794},
	file = {Chen_Guestrin_2016_XGBoost.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Chen_Guestrin_2016_XGBoost.pdf:application/pdf}
}

@inproceedings{bradley_filterboost:_2007,
	address = {USA},
	series = {{NIPS}'07},
	title = {{FilterBoost}: {Regression} and {Classification} on {Large} {Datasets}},
	isbn = {978-1-60560-352-0},
	shorttitle = {{FilterBoost}},
	url = {http://dl.acm.org/citation.cfm?id=2981562.2981586},
	abstract = {We study boosting in the filtering setting, where the booster draws examples from an oracle instead of using a fixed training set and so may train efficiently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, \& Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the first proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the filtering setting. Our proofs demonstrate the algorithm's strong theoretical properties for both classification and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overfitting than batch boosters in conditional probability estimation and proves competitive in classification.},
	urldate = {2018-04-29},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Bradley, Joseph K. and Schapire, Robert E.},
	year = {2007},
	pages = {185--192},
	file = {Bradley_Schapire_2007_FilterBoost.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Bradley_Schapire_2007_FilterBoost.pdf:application/pdf}
}

@incollection{ke_lightgbm:_2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	shorttitle = {{LightGBM}},
	url = {http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf},
	urldate = {2018-04-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3146--3154},
	file = {Ke_et_al_2017_LightGBM.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Ke_et_al_2017_LightGBM.pdf:application/pdf;NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/HPYX3H3G/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.html:text/html}
}

@book{owen_monte_2013,
	title = {Monte {Carlo} {Theory}, {Methods} and {Examples}},
	url = {http://statweb.stanford.edu/~owen/mc/},
	author = {Owen, Art B.},
	year = {2013}
}

@inproceedings{mcsherry_scalability!_2015,
	address = {Kartause Ittingen, Switzerland},
	title = {Scalability! {But} at what {COST}?},
	url = {https://www.usenix.org/conference/hotos15/workshop-program/presentation/mcsherry},
	booktitle = {15th {Workshop} on {Hot} {Topics} in {Operating} {Systems} ({HotOS} {XV})},
	publisher = {USENIX Association},
	author = {McSherry, Frank and Isard, Michael and Murray, Derek G.},
	year = {2015},
	file = {Scalability! But at what COST  USENIX.pdf:/Users/arapat/Documents/Geisel/storage/FNMRSIUH/Scalability! But at what COST  USENIX.pdf:application/pdf;Scalability! But at what COST? | USENIX:/Users/arapat/Documents/Geisel/storage/YLQJS76A/mcsherry.html:text/html}
}

@article{zaharia_apache_2016,
	title = {Apache {Spark}: {A} {Unified} {Engine} for {Big} {Data} {Processing}},
	volume = {59},
	issn = {0001-0782},
	shorttitle = {Apache {Spark}},
	url = {http://doi.acm.org/10.1145/2934664},
	doi = {10.1145/2934664},
	abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
	number = {11},
	urldate = {2018-04-29},
	journal = {Commun. ACM},
	author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
	month = oct,
	year = {2016},
	pages = {56--65},
	file = {Zaharia_et_al_2016_Apache_Spark.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/distributed-learning-in-general/Zaharia_et_al_2016_Apache_Spark.pdf:application/pdf}
}

@incollection{recht_hogwild:_2011,
	title = {Hogwild: {A} {Lock}-{Free} {Approach} to {Parallelizing} {Stochastic} {Gradient} {Descent}},
	shorttitle = {Hogwild},
	url = {http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf},
	urldate = {2018-04-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
	publisher = {Curran Associates, Inc.},
	author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	pages = {693--701},
	file = {NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/ED3YXDWM/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.html:text/html;Recht_et_al_2011_Hogwild.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/optimization/Recht_et_al_2011_Hogwild.pdf:application/pdf}
}

@article{caragea_framework_2004,
	title = {A {Framework} for {Learning} from {Distributed} {Data} {Using} {Sufficient} {Statistics} and {Its} {Application} to {Learning} {Decision} {Trees}},
	volume = {1},
	issn = {1448-5869},
	url = {https://content.iospress.com/articles/international-journal-of-hybrid-intelligent-systems/his010},
	doi = {10.3233/HIS-2004-11-210},
	abstract = {This paper motivates and precisely formulates the problem of learning from distributed data; describes a general strategy for transforming traditional machine learning algorithms into algorithms for learning from distributed data; demonstrates the ap},
	language = {en},
	number = {1-2},
	urldate = {2018-04-29},
	journal = {International Journal of Hybrid Intelligent Systems},
	author = {Caragea, Doina and Silvescu, Adrian and Honavar, Vasant},
	month = jan,
	year = {2004},
	pages = {80--89},
	file = {Snapshot:/Users/arapat/Documents/Geisel/storage/7Z72LX4S/his010.html:text/html}
}

@inproceedings{viola_rapid_2001,
	title = {Rapid {Object} {Detection} using a {Boosted} {Cascade} of {Simple} {Features}},
	volume = {1},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	author = {Viola, P. and Jones, M.},
	year = {2001},
	keywords = {AdaBoost, background regions, boosted simple feature cascade, classifiers, Detectors, face detection, Face detection, feature extraction, Filters, Focusing, image classification, image processing, image representation, Image representation, integral image, learning (artificial intelligence), machine learning, Machine learning, object detection, Object detection, object specific focus-of-attention mechanism, Pixel, rapid object detection, real-time applications, Robustness, Skin, statistical guarantees, visual object detection},
	pages = {I--511--I--518 vol.1},
	file = {IEEE Xplore Abstract Record:/Users/arapat/Documents/Geisel/storage/7TDQWLWB/990517.html:text/html}
}

@book{schapire_boosting:_2012,
	title = {Boosting: {Foundations} and {Algorithms}},
	isbn = {978-0-262-01718-3},
	shorttitle = {Boosting},
	language = {en},
	publisher = {MIT Press},
	author = {Schapire, Robert E. and Freund, Yoav},
	year = {2012},
	note = {Google-Books-ID: blSReLACtToC},
	keywords = {Computers / Machine Theory, Computers / Programming / Algorithms}
}

@inproceedings{mason_boosting_1999,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'99},
	title = {Boosting {Algorithms} {As} {Gradient} {Descent}},
	url = {http://dl.acm.org/citation.cfm?id=3009657.3009730},
	abstract = {We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.},
	urldate = {2018-05-03},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},
	year = {1999},
	pages = {512--518}
}

@article{hoeffding_probability_1963,
	title = {Probability {Inequalities} for {Sums} of {Bounded} {Random} {Variables}},
	volume = {58},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2282952},
	doi = {10.2307/2282952},
	abstract = {Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for {\textless}tex-math{\textgreater}\${\textbackslash}Pr {\textbackslash}\{ S - ES {\textbackslash}geq nt {\textbackslash}\}\${\textless}/tex-math{\textgreater} depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population.},
	number = {301},
	urldate = {2018-05-03},
	journal = {Journal of the American Statistical Association},
	author = {Hoeffding, Wassily},
	year = {1963},
	pages = {13--30},
	file = {Hoeffding_1963_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/statistics/Hoeffding_1963_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables.pdf:application/pdf;Snapshot:/Users/arapat/Documents/Geisel/storage/WBBPID6J/01621459.1963.html:text/html}
}

@article{valiant_bridging_1990,
	title = {A {Bridging} {Model} for {Parallel} {Computation}},
	volume = {33},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/79173.79181},
	doi = {10.1145/79173.79181},
	abstract = {The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.},
	number = {8},
	urldate = {2018-05-17},
	journal = {Commun. ACM},
	author = {Valiant, Leslie G.},
	month = aug,
	year = {1990},
	pages = {103--111},
	file = {Valiant_1990_A_Bridging_Model_for_Parallel_Computation.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/synchronous/Valiant_1990_A_Bridging_Model_for_Parallel_Computation.pdf:application/pdf}
}

@inproceedings{liu_asynchronous_2014,
	title = {An {Asynchronous} {Parallel} {Stochastic} {Coordinate} {Descent} {Algorithm}},
	url = {http://proceedings.mlr.press/v32/liud14.html},
	abstract = {We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on...},
	language = {en},
	urldate = {2018-05-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Liu, Ji and Wright, Steve and Re, Christopher and Bittorf, Victor and Sridhar, Srikrishna},
	month = jan,
	year = {2014},
	pages = {469--477},
	file = {Liu_et_al_2014_An_Asynchronous_Parallel_Stochastic_Coordinate_Descent_Algorithm.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/asynchronous/Liu_et_al_2014_An_Asynchronous_Parallel_Stochastic_Coordinate_Descent_Algorithm.pdf:application/pdf;Snapshot:/Users/arapat/Documents/Geisel/storage/XH8VGTNX/liud14.html:text/html}
}

@incollection{lian_asynchronous_2015,
	title = {Asynchronous {Parallel} {Stochastic} {Gradient} for {Nonconvex} {Optimization}},
	url = {http://papers.nips.cc/paper/5751-asynchronous-parallel-stochastic-gradient-for-nonconvex-optimization.pdf},
	urldate = {2018-05-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2737--2745},
	file = {Lian_et_al_2015_Asynchronous_Parallel_Stochastic_Gradient_for_Nonconvex_Optimization.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/asynchronous/Lian_et_al_2015_Asynchronous_Parallel_Stochastic_Gradient_for_Nonconvex_Optimization.pdf:application/pdf;NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/47ZX6T2C/5751-asynchronous-parallel-stochastic-gradient-for-nonconvex-optimization.html:text/html}
}