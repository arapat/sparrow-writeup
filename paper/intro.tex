\section{Introduction}\label{sec:intro}

It is well known that I/O is the main bottleneck to the application of
machine learning algorithms to very large datasets. Improvements to
date organization on disk, memory and cache for ML algorithms are
active areas of research.(add refs here).

Many machine learning algorithms, including XGBoost,
follow the a similar pattern. At each iteration the complete training
set is scanned. Then, based on statistics calculated from the training
set, the model is updated to decrease the loss.

Scanning all of the data is obviously optimal from the point of view
of the statistical estimates. However, the incremental improvemen in
the model, given an increase from a fracton of the data to the full
data, might be small. If it is sufficiently small, then stopping early
and proceeding to the next iteration can result in significantly
faster running time.

This idea was introduced by Wald\cite{wald_sequential_1973} in the
1940s under the title ``Sequential analysis'' (SA). We describe SA
precisely in Section~\ref{sec:sequential-analysis}. As an illustrative
example, suppose that our goal is to find a classifier from
$M_1,\ldots,M_k$ whose error rate is close to the minimum across that
$k$ models. The standard approach is to scan all of the training
examples, compute the average error of each model, and choose the
model with the minimal error. The SA approach is to read the examples
one by one, each time updating the error estimate for each model, and
emply a specifically designed {\em stopping rule} to decide when to
stop and which model to output. An example where SA will stop much
before the standard approach is when one rule has an error rate of
$0.1$ while all of the other rules have error of $0.5$.

The use of stopping rules in boosting algorithms is not new, it was
studied before by Domingo and Watanabe~\cite{domingo_scaling_2000} and
by Bradley and Schapire~\cite{bradley_filterboost:_2007}.

Our main contribution in this paper a new parallel boosting algorithm
that takes advantage of early stopping. Observe that the main
computation step of the boosting algorithm is the search for a rule
that is slightly better than random guessing.~\footnote{Often the
  stated goal is to find the {\em best} weak rule. However, for the
  theory behind adaboost to hold, any rule whose error is smaller than
  $1/2$ will do.}

Both XGBoost and LightGBM are based on the
Bulk-Synchronization~\ref{valiant_bridging_1990} model of parallel
computation. In this model the workers are all synchronized at the
start of each boosting iteration. This establishes a well defined {\em
  common state} at the synchronization boundary which simplifies the
reasoning about the algorithm an it's interaction with the hardware
and the OS.

In our approach, which we call ``tell me something new'' or
\tmsn\ {\em we do away with both synchronization and common state}.

We first give a rough description of \tmsn\, followed with a more
refined one. Roughly, each of the worker is streaming over it's local
data to evaluate the error of a set of weak rules. It uses a stopping
rule to stop when it identifies a weak rule whose error is smaller
than $1/2-\gamma$. when this good weak rule has been identified, it
interrupts the search, adds the found rule to the strong rule, and
broadcasts this strong rule. The other workers, upon recieving the new
strong rule, interrupt their own search and use the new rule.

This rough version of \tmsn\ would work if the workers were
synchronized and communication was instantanous. Both are unrealistic
assumptions. To remove the need for these assumptions we add a {\em
  global measure of progress}, which is an upper bound on the expected
loss of the strong hypothesis. When a worker broadcasts a new strong
rule, it pairs it with this upper bound. When a worker recieves a
strong rule, it acccepts it only if the newcomers upper bound is
smaller than the current upper bound. The full details are in
section~\ref{sec:}

We call our protocol ``tell me something new'' because the workers
send out information only when they have ``something new'' which in
our case means a significantly better strong rule. This protocol has
several desirable properties:
\begin{enumerate}
\item {\bf No head-node} Most distributed systems rely on a head-node
  that synchronizes the workers. The head node is a
  single-point-of-failure and a bottleneck, especially systems with a
  large number of workers. \tmsn\ avoids these problems because it
  does not require a head-node.
\item {\bf Reduced communication bandwidth:} In typical bulk-synchronous
  protocols all workers send and recieve information from the head
  node at each step. This can mean a lot of communication even when
  there is little progress in terms of reducing loss. The workers in
  \tmsn\ remain mute when as long as they don't find anything useful.
\item {\bf No blocking} The communication operations are all
  non-blocking, in other words, at no point does a worker wait for a
  response from another worker. This means that CPU utilzation is not
  reduce by wait times.
\item {\bf Robustness} As a result of the fact that there is no
  synchronization and no head node, they system is very robust. If a
  single computer is slow or crashes, the effect on the other
  computers is small. Computers can be removed or added at any time
  and will quickly catch up to the current best model and start
  contributing their cycles to the search.
\end{enumerate}

\iffalse
A data structure $M$ defines the model, the
algorithm compares several alternative models $M_1,\ldots,M_k$. Ideally,
models should be compared by expected loss, defined as 
$$\loss(M) = E_{\vx \sim \Dist}(L(M,\vx))$$
where $\vx$ is an example chosen independently at random according to
the distribution $\Dist$ and $L(M,\vx)$ is a loss function which associates a
real number with the application of a model $M$ to the example $\vx$.
For reasons that will be revealed shortly, we assume that the range of
$L$ is $[0,1]$.

We refer to the expected loss $\loss(M)$ as the {\em ideal} because in
order to calculate it we need an infinite number of example. As in
practice the number of examples is finite, the algorithm has to use an {\em estimate}.

We denote a model by $M$ and a data point by $\vx$. The datapoints are
generated by a fixed but unknown distribution $\Dist$. A loss function $L$
assigns a real valued to a model $M$ applied to a datapoint $\vx$:
$L(M,\vx)$. We restrict our losses to the values $[0,1]$.

The quantity that we wish to estimate is the expected loss with
respect to the true distribution $\Dist$: $\loss(M) = E_{\vx \sim
  \Dist}(L(M,\vx))$. We cannot caculate $\loss(M)$, we can only
{\em estimate} it from our finite traning data: $T = \{ \vx_1, \ldots,
\vx_n\}$. The most common estimator is the unweighted average:
$$ \eloss_n(M) = \frac{1}{n} \sum_{i=1}^n L(M,\vx_i)$$




Most of this work ignores a fundamental charasteristic
of machine learning tasks - the bias-variance tradeoff. Most machine
learning algorithm consider only the {\em mean} of the loss

On the other hand, computer clock rates are unlikely to increase
beyond 4\,GHz in the foreseeable future.  As a result there is a keen
interest in parallelized machine
learning algorithms~\cite{bekkerman_scaling_2012}.

The most common approach to parallel ML is based on Valiant's bulk
synchronous~\cite{valiant_bridging_1990} model. This approach calls for a
set of workers and a master. The system works in (bulk) iterations. In each iteration
the master sends a task to each worker and then waits for its
response. Once {\em all} machines responded, the master proceeds to the
next iteration. Thus the head node enforces synchronization (at the
iteration level) and maintains
a state that is shared by all of the workers.

Unfortunately, bulk synchronization does not scale well to more than
10--20 computers. Network congestion, latencies due to synchronization,
laggards, and failing computers result in diminishing benefits from
adding more workers to the
cluster~\cite{zaharia_apache_2016,mcsherry_scalability!_2015}.

There have been several attempts to break out of the bulk-synchronized
framework, most notably the work of Recht et~al.\ on
Hogwild~\cite{recht_hogwild:_2011} and Lian et~al.\ on asynchronous
stochastic descent~\cite{lian_asynchronous_2015}. Hogwild
significantly reduces the synchronization penalty by using
asynchronous updates and parameter servers. The basic idea is to
decentralize the task of maintaining a global state and relying on
sparse updates to limit the frequency of update clashes.

\paragraph{Tell Me Something New}

Our first contribution is a new approach for parallelizing ML algorithms
which eliminates synchronization and the global state and instead uses a
distributed policy that guarantees progress. We call this approach
``Tell Me Something New'' (\tmsn). To explain \tmsn\ we start with an
analogy.

Consider a team of a hundred investigators that is going through
thousands of documents to build a criminal case where time is of the
issue. Assume also that most of the documents contain little or no new
information. How should the investigators communicate their findings
with each other? We contrast the bulk-synchronous (BS) approach and the
\tmsn\ approach. In the BS approach, each investigator takes a stack
of documents to their cubicle and reads through it. Then all of the
investigator meet in a room and tell each other what they found. Once
they are done, the process repeats. One problem with this approach is
that the fast readers have to wait for the slow readers. Another is
that a decision needs to be made as to how many documents or pages, to
put in each stack. Too many and the iterations would be very slow, too
few and all of the time would be spent in meetings.

The \tmsn\ approach is radically different. In this approach, each
investigator gets documents independently according to their speed of
reading and work habits. There is no meeting either. Instead, when
an investigator finds a piece of information that she believes is new,
she stands up in her cubicle and tells all of the other workers about
it. This has several advantages: nobody is ever waiting for anybody
else; the new information is broadcasted as soon as it is available, and
the system is fault resilient --- somebody falling asleep has little
effect on the others.
The analogy to parallel ML maps investigators
to computers, ``case'' to ``model'', and ``new information'' to
``improved model''.

%\iffalse
Consider a team of investigators that is going through
thousands of documents to build a criminal case. Assume also that most
of the documents contain no new information. It makes little sense for
them to send each other a summary of each document they read, they
would just be wasting each other's time that way. What would make more
sense is for each to sit reading in their cubicle until one of them
identifies a document with new information, that person than stands
up, interrupts all of the other people, and gives each of them a
summary of what they found. This ensures that all of the investigators
are updated as soon as new information is available, but are otherwise
left to do their work.
%\fi

More concretely, \tmsn\ for model learning works as follows. Each
worker has a model $H$ and an upper bound $L$ on the true loss of
$H$. The worker searches for a better model $H'$ whose loss upper
bound is $L'$. If $L'$ is significantly smaller than $L$, then the
worker takes two actions. First, $H',L'$ replaces $H,L$. Second
$(H',L')$ is broadcast to all other workers.  Each worker also listens
to the broadcast channel. If it receives pair $(H',L')$ it checks
whether $L'$ is significantly lower than its own upper bound $L$. If
it is, the worker replaces $(H,L)$ with $(H',L')$. Otherwise, the worker discards the
pair.

\paragraph{Boosting trees using TMSN}
Our second contribution is an application of \tmsn\ to boosted
decision trees. Boosted trees is a highly effective and widely used
machine learning method. In recent years there have been several
implementations of boosting that greatly improve over previous
implementations in terms of running time, in particular,
XGBoost~\cite{chen_xgboost:_2016} and
LightGBM~\cite{ke_lightgbm:_2017}. These implementations scale up to
training sets of many millions, or even billions of training examples.
Both implementations can run in one of two configurations: a
memory-only configuration where all of the training data is stored in
main memory, and a memory and disk configuration where the data is on
disk and is copied into memory when needed. The memory-only version is
significantly faster, but require a machine with very large
memory.

We present an implementation of boosting tree learning using
\tmsn\ that we call \Sparrow. This is a disk and memory
implementation, which requires only a fraction of the training data to
be stored in memory.  Yet, as our comparative experiments show, it is
about 10 times faster than XGBoost and LightGBM using the {\em memory
  only} configuration.

The rest of the paper is divided into four sections.
First we give a general description of \tmsn\ in Section~\ref{sec:tmsn}.
Then we introduce a special application of our algorithm, namely \Sparrow, in Section~\ref{sec:boost}.
After that we describe in more details of the algorithms and the system design of \Sparrow\ in
Section~\ref{sec:Algorithms}.
Finally, we present empirical results in Section~\ref{sec:experiments}.
\fi
