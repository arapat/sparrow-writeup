\section{Introduction}\label{sec:intro}
A major bottleneck for learning algorithms that use big data is the
transfer of the training data up the memory hierarchy from disk,
through main memory and the caches to the CPU cores. This problem
becomes severe when the training data does not fit in main memory and
examples have to be processed multiple times.

The leading implementations of boosted trees: XGBoost~\cite{chen_xgboost:_2016} and
LightGBM~\cite{ke_lightgbm:_2017} suffer from this problem. LightGBM
will not run if memory is not sufficiently large to hold all of the
training data. XGBoost will run in that case, but is very slow.

The main contribution of this paper is a boosting tree algorithm we
call \Sparrow, which runs much faster than XGBoost and LightGBM,
especially when memory is too small to store all of the training data.

Our initial implementation, presented here, is for the special case of
boosting ``stumps'', i.e. trees of depth one. Plans for extending the
work to full-size trees are described at the end of the paper.

A trivial way to deal with the problem of limited memory is to read
into memory as much data as would fit, and learn only from that data.
The problem with that approach is increased over-fitting. One approach
to this problem is presented in random-forests~\cite{hastie_elements_2009},
where trees are trained on separate subsets of the data and then
combined by a majority vote. Random forests significantly reduce the
variance of the trees, but they do not decrease the bias. Boosting can
reduce the variance as well as the bias~\cite{schapire_boosting_1998}. We
therefor want a memory-efficient version of boosting.

\Sparrow\ is based on three ideas from statistics: {\em early stopping},
the {\em effective} number of examples and {\em weighted
  sampling}. Each of these ideas appears in the literature, but their
combination into an algorithm is new. We now describe how these three
ideas are combined.

At a high level, \Sparrow\ works as follows. It repeatedly draws
weighted samples from disk into memory. Each sample is
used for multiple boosting iterations. As boosting progresses the
weights of the examples become increasingly skewed. A skewed sample is
effectively smaller. Examples with very small weight are essentially a
waste of memory space. We quantify this by a quantity we call ``the
effective number of examples'. When the  ``the effective number
of examples'' decreases below a threshold \Sparrow\ flushes the sample
and selects a new sample. The new sample have uniform weights and
therefor the effective sample size is equal to the size of the sample.

As for reading examples from memory into the CPU, {\em early stopping}
is used to stop the reading of examples as soon as a good enough weak
rule has been identified.

We implemented \Sparrow\ using the Rust programming language. We
compared it's performance to the performance of XGBoost and LightGBM on
50 Million examples (the human acceptor splice site
dataset~\cite{sonnenburg_coffin_2010, agarwal_reliable_2014}). We
show that on a 16 GB machine \Sparrow\ is almost 200 times faster than
XGBoost. On 144 GB machine all of the data can be loaded into memory
and the performance of \Sparrow\ is 6-13x faster than LightGBM and XGBoost.

The rest of the paper is organized as follows. In Section~\ref{sec:theory} we
describe the statistical tools used in the design of \Sparrow. In
Section~\ref{sec:Algorithms} we describe \Sparrow. In Section~\ref{sec:experiments} we describe
the experiments comparing \Sparrow\ to XGBoost and LightGBM. We
conclude in Section~\ref{sec:Conclusion} with %some conclusions and
future work.

\section{related work}

The amount of published work on boosting is very large, and reviewing
it is out of our scope. We highlight the work that most directly
relates to our work here.

We developed \Sparrow\ using analysis based on the original formulation of
boosting~\cite{freund_alternating_1999, schapire_improved_1999,
  freund_alternating_1999, schapire_boosting:_2012}. We give a brief
review of this formulation to help the reader understand our analysis.

We compare \Sparrow\ to the current two leading implementations
of boosting trees: XGBoost~\cite{chen_xgboost:_2016} and
LightGBM~\cite{ke_lightgbm:_2017}.
These implementations are based on an alternative formulation of
boosting propsed by Friedman Hastie and
Tibshirani~\cite{friedman_additive_2000}.

The use of stopping rules to accelerate boosting has been studied 
before us by Domingo and Watanabe~\cite{domingo_scaling_2000} and by
Bradley and Schapire~\cite{bradley_filterboost:_2007}.


\iffalse

There are two main ways of using the example weights in boosting:
\begin{itemize}
\item {\bf Boosting by filtering}: Examples arrive in a stream and are
  selected with probability proportional to their weight. The weighted
  training error is calculated using an {\em unweighted average} over
  the selected examples.
\item {\bf Boosting by weighting}: Examples are stored in memory,
  along with their weight. The weighted training error is calculated
  by taking the weighted average over all of the examples.
\end{itemize}
The algorithm described
in~\cite{domingo_scaling_2000,bradley_filterboost:_2007} are both
based on {\em boosting by filtering}. This means that only one example
needs to be stored in memory at any time. That extremely small memory
footprint comes at the cost of under-utilizing the memory and putting
undue burden on disk-to-memory communication.

Our approach is to improve memory utilization by combining boosting by
filtering and boosting by weighting,
which we call \Sparrow.
It periodically samples
batches on examples using boosting-by-filtering. It then performs several
boosting-by-weighting iterations using the memory-resident batches.

The weights of the memory resident batch are initially uniform. These
weights change into non-uniform weights as boosting-by-weighting
progresses. This decreases the ``effective size'' of the batch. In
Section~\ref{} we show that a good measure of the effective size of a
set of examples with weights $w_1,\ldots,w_n$ is $(\sum_i w_i)^2 /
(\sum_i w_i^2)$. Having a small effective size implies a large error
in the estimates of the weighted error. When the effective size falls
below a set thresholds, the batch is discarded and a new batch is
filtered to replace it. The new batch will always have effective size
$n$. By reusing the data in memory and sampling a new batch only when
needed, \Sparrow\ achieves significantly faster performance than
other algorithm.

Additional speedup is achieve by using a stopping rule when
running boosting on the memory-resident batch. This stopping rule is
similar to the stopping rules
used~\cite{domingo_scaling_2000,bradley_filterboost:_2007} with the
additional refinement that it takes in weights and the effective
sample size into account. We describe this stopping rule in Section~\ref{}.
\fi
