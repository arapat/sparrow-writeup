\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{}
\citation{sonnenburg_coffin_2010,agarwal_reliable_2014}
\citation{}
\citation{chen_xgboost:_2016}
\citation{ke_lightgbm:_2017}
\citation{a}
\citation{}
\citation{wald_sequential_1973}
\citation{domingo_scaling_2000}
\citation{bradley_filterboost:_2007}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\citation{domingo_scaling_2000,bradley_filterboost:_2007}
\citation{}
\citation{}
\newlabel{sec:tmsn}{{2}{2}{}{section.2}{}}
\citation{schapire_boosting:_2012}
\citation{mason_boosting_1999}
\newlabel{eqn:hoeffding}{{1}{3}{}{equation.2.1}{}}
\newlabel{fig:async}{{1}{3}{{\bf Execution timeline of a \tmsn \ system} System consists of four workers. The first update occurs when worker 3 identifies a better classifier $H_1$. It then replaces $H_0$ with $H_1$ and broadcasts $(H_1,z_1)$ to the other workers. The other workers receive the message the at different times, depending on network congestion. At that time they interrupt the scanner (yellow explosions) and start using $H_1$. Next, worker 2 identifies an improved rule $H_2$ and the same process ensues}{figure.1}{}}
\newlabel{eqn:gamma_emp}{{2}{3}{}{equation.3.2}{}}
\citation{bradley_filterboost:_2007}
\citation{domingo_scaling_2000}
\citation{bradley_filterboost:_2007}
\citation{domingo_scaling_2000}
\citation{balsubramani_sharp_2014}
\citation{balsubramani_sharp_2014}
\citation{owen_monte_2013}
\citation{hoeffding_probability_1963}
\newlabel{eqn:gamma_emp}{{3}{4}{}{equation.3.3}{}}
\newlabel{sec:methods:early-stop}{{3}{4}{}{equation.3.3}{}}
\newlabel{thm:balsubramani}{{1}{4}{based on \cite {balsubramani_sharp_2014} Theorem 4}{theorem.1}{}}
\newlabel{sec:effectiveSampleSize}{{3}{4}{}{theorem.1}{}}
\newlabel{eqn:Ztest}{{4}{4}{}{equation.3.4}{}}
\newlabel{eqn:neff}{{5}{4}{}{equation.3.5}{}}
\citation{caragea_framework_2004}
\citation{kitagawa_monte_1996}
\newlabel{fig:architecture}{{2}{5}{The \Sparrow \ system architecture}{figure.2}{}}
\newlabel{sec:Algorithms}{{4}{5}{}{section.4}{}}
\newlabel{sec:single_worker}{{4.1}{5}{}{subsection.4.1}{}}
\citation{sonnenburg_coffin:_2010,agarwal_reliable_2014}
\newlabel{sec:experiments}{{5}{6}{Incremental Updates:}{section.5}{}}
\newlabel{table-exp}{{5}{6}{Incremental Updates:}{section.5}{}}
\bibdata{ms}
\bibcite{agarwal_reliable_2014}{{1}{2014}{{Agarwal et~al.}}{{Agarwal, Chapelle, Dud√≠k, and Langford}}}
\bibcite{balsubramani_sharp_2014}{{2}{2014}{{Balsubramani}}{{}}}
\bibcite{bradley_filterboost:_2007}{{3}{2007}{{Bradley \& Schapire}}{{Bradley and Schapire}}}
\bibcite{caragea_framework_2004}{{4}{2004}{{Caragea et~al.}}{{Caragea, Silvescu, and Honavar}}}
\bibcite{chen_xgboost:_2016}{{5}{2016}{{Chen \& Guestrin}}{{Chen and Guestrin}}}
\bibcite{domingo_scaling_2000}{{6}{2000}{{Domingo \& Watanabe}}{{Domingo and Watanabe}}}
\bibcite{hoeffding_probability_1963}{{7}{1963}{{Hoeffding}}{{}}}
\bibcite{ke_lightgbm:_2017}{{8}{2017}{{Ke et~al.}}{{Ke, Meng, Finley, Wang, Chen, Ma, Ye, and Liu}}}
\bibcite{kitagawa_monte_1996}{{9}{1996}{{Kitagawa}}{{}}}
\newlabel{fig:loss}{{3}{7}{Comparing the average loss on the testing data using \Sparrow , XGBoost, and LightGBM, lower is better. The period of time that the loss is constant for \Sparrow \ is when the algorithm is generating a new sample set}{figure.3}{}}
\newlabel{fig:auprc}{{4}{7}{Comparing the area under the precision-recall curve (AUPRC) on the testing data using \Sparrow , XGBoost, and LightGBM, higher is better. (left) Normal scale, clipped on right. (right) Log scale, clipped on left. The period of time that the AUPRC is constant for \Sparrow \ is when the algorithm is generating a new sample set}{figure.4}{}}
\bibcite{mason_boosting_1999}{{10}{1999}{{Mason et~al.}}{{Mason, Baxter, Bartlett, and Frean}}}
\bibcite{owen_monte_2013}{{11}{2013}{{Owen}}{{}}}
\bibcite{schapire_boosting:_2012}{{12}{2012}{{Schapire \& Freund}}{{Schapire and Freund}}}
\bibcite{sonnenburg_coffin:_2010}{{13}{2010}{{Sonnenburg \& Franc}}{{Sonnenburg and Franc}}}
\bibcite{wald_sequential_1973}{{14}{1973}{{Wald}}{{}}}
\bibstyle{sysml2019}
