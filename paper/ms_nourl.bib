
@inproceedings{domingo_scaling_2000,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Scaling {Up} a {Boosting}-{Based} {Learner} via {Adaptive} {Sampling}},
	isbn = {978-3-540-67382-8 978-3-540-45571-4},
	doi = {10.1007/3-540-45571-X_37},
	abstract = {In this paper we present a experimental evaluation of a boosting based learning system and show that can be run efficiently over a large dataset. The system uses as base learner decision stumps, single atribute decision trees with only two terminal nodes. To select the best decision stump at each iteration we use an adaptive sampling method. As a boosting algorithm, we use a modification of AdaBoost that is suitable to be combined with a base learner that does not use all the dataset. We provide experimental evidence that our method is as accurate as the equivalent algorithm that uses all the dataset but much faster.},
	language = {en},
	urldate = {2018-04-26},
	booktitle = {Knowledge {Discovery} and {Data} {Mining}. {Current} {Issues} and {New} {Applications}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Domingo, Carlos and Watanabe, Osamu},
	month = apr,
	year = {2000},
	pages = {317--328},
	file = {Domingo_Watanabe_2000_Scaling_Up_a_Boosting-Based_Learner_via_Adaptive_Sampling.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Domingo_Watanabe_2000_Scaling_Up_a_Boosting-Based_Learner_via_Adaptive_Sampling.pdf:application/pdf}
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	number = {1},
	urldate = {2018-04-27},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
	file = {Freund_Schapire_1997_A_Decision-Theoretic_Generalization_of_On-Line_Learning_and_an_Application_to.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Freund_Schapire_1997_A_Decision-Theoretic_Generalization_of_On-Line_Learning_and_an_Application_to.pdf:application/pdf;ScienceDirect Snapshot:/Users/arapat/Documents/Geisel/storage/5P373W7Y/S002200009791504X.html:text/html}
}

@incollection{abuzaid_yggdrasil:_2016,
	title = {Yggdrasil: {An} {Optimized} {System} for {Training} {Deep} {Decision} {Trees} at {Scale}},
	shorttitle = {Yggdrasil},
	urldate = {2018-04-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Abuzaid, Firas and Bradley, Joseph K and Liang, Feynman T and Feng, Andrew and Yang, Lee and Zaharia, Matei and Talwalkar, Ameet S},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {3817--3825},
	file = {Abuzaid_et_al_2016_Yggdrasil.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/synchronous/Abuzaid_et_al_2016_Yggdrasil.pdf:application/pdf;NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/SSH2CX9M/6366-yggdrasil-an-optimized-system-for-training-deep-decision-trees-at-scale.html:text/html}
}

@article{kitagawa_monte_1996,
	title = {Monte {Carlo} {Filter} and {Smoother} for {Non}-{Gaussian} {Nonlinear} {State} {Space} {Models}},
	volume = {5},
	doi = {10.1080/10618600.1996.10474692},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Kitagawa, Genshiro},
	year = {1996},
	pages = {1--25},
	file = {Kitagawa_1996_Monte_Carlo_Filter_and_Smoother_for_Non-Gaussian_Nonlinear_State_Space_Models.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/statistics/Kitagawa_1996_Monte_Carlo_Filter_and_Smoother_for_Non-Gaussian_Nonlinear_State_Space_Models.pdf:application/pdf}
}

@article{balsubramani_sharp_2014,
	title = {Sharp {Finite}-{Time} {Iterated}-{Logarithm} {Martingale} {Concentration}},
	abstract = {We give concentration bounds for martingales that are uniform over finite times and extend classical Hoeffding and Bernstein inequalities. We also demonstrate our concentration bounds to be optimal with a matching anti-concentration inequality, proved using the same method. Together these constitute a finite-time version of the law of the iterated logarithm, and shed light on the relationship between it and the central limit theorem.},
	urldate = {2018-04-29},
	journal = {arXiv:1405.2639 [cs, math, stat]},
	author = {Balsubramani, Akshay},
	month = may,
	year = {2014},
	keywords = {60E15, 60G17 (Primary), 60G40, 60G42, 60G44 (Secondary), Computer Science - Learning, Mathematics - Probability, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/arapat/Documents/Geisel/storage/YPDIT4BQ/1405.html:text/html;Balsubramani_2014_Sharp_Finite-Time_Iterated-Logarithm_Martingale_Concentration.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/sequential-analysis/Balsubramani_2014_Sharp_Finite-Time_Iterated-Logarithm_Martingale_Concentration.pdf:application/pdf}
}

@inproceedings{sonnenburg_coffin_2010,
	address = {USA},
	series = {{ICML}'10},
	title = {{COFFIN}: {A} {Computational} {Framework} for {Linear} {SVMs}},
	isbn = {978-1-60558-907-7},
	shorttitle = {{COFFIN}},
	abstract = {In a variety of applications, kernel machines such as Support Vector Machines (SVMs) have been used with great success often delivering state-of-the-art results. Using the kernel trick, they work on several domains and even enable heterogeneous data fusion by concatenating feature spaces or multiple kernel learning. Unfortunately, they are not suited for truly large-scale applications since they suffer from the curse of supporting vectors, i.e., the speed of applying SVMs decays linearly with the number of support vectors. In this paper we develop COFFIN — a new training strategy for linear SVMs that effectively allows the use of on demand computed kernel feature spaces and virtual examples in the primal. With linear training and prediction effort this framework leverages SVM applications to truly large-scale problems: As an example, we train SVMs for human splice site recognition involving 50 million examples and sophisticated string kernels. Additionally, we learn an SVM based gender detector on 5 million examples on low-tech hardware and achieve beyond the state-of-the-art accuracies on both tasks. Source code, data sets and scripts are freely available from http://sonnenburgs.de/soeren/coffin.},
	urldate = {2018-04-29},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Sonnenburg, Soeren and Franc, Vojtěch},
	year = {2010},
	pages = {999--1006},
	file = {Sonnenburg_Franc_2010_COFFIN.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/bioinformatics/Sonnenburg_Franc_2010_COFFIN.pdf:application/pdf}
}

@book{bekkerman_scaling_2012,
	title = {Scaling {Up} {Machine} {Learning}: {Parallel} and {Distributed} {Approaches}},
	isbn = {978-0-521-19224-8},
	shorttitle = {Scaling {Up} {Machine} {Learning}},
	abstract = {This book presents an integrated collection of representative approaches for scaling up machine learning and data mining methods on parallel and distributed computing platforms. Demand for parallelizing learning algorithms is highly task-specific: in some settings it is driven by the enormous dataset sizes, in others by model complexity or by real-time performance requirements. Making task-appropriate algorithm and platform choices for large-scale machine learning requires understanding the benefits, trade-offs, and constraints of the available options. Solutions presented in the book cover a range of parallelization platforms from FPGAs and GPUs to multi-core systems and commodity clusters, concurrent programming frameworks including CUDA, MPI, MapReduce, and DryadLINQ, and learning settings (supervised, unsupervised, semi-supervised, and online learning). Extensive coverage of parallelization of boosted trees, SVMs, spectral clustering, belief propagation and other popular learning algorithms and deep dives into several applications make the book equally useful for researchers, students, and practitioners.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Bekkerman, Ron and Bilenko, Mikhail and Langford, John},
	year = {2012},
	keywords = {Computers / Computer Vision \& Pattern Recognition, Computers / General, Computers / Intelligence (AI) \& Semantics, Computers / Optical Data Processing},
	file = {Bekkerman_et_al_2012_Scaling_Up_Machine_Learning.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/large-scale-learning/Bekkerman_et_al_2012_Scaling_Up_Machine_Learning.pdf:application/pdf}
}

@article{agarwal_reliable_2014,
	title = {A {Reliable} {Effective} {Terascale} {Linear} {Learning} {System}},
	volume = {15},
	journal = {Journal of Machine Learning Research},
	author = {Agarwal, Alekh and Chapelle, Oliveier and Dudík, Miroslav and Langford, John},
	year = {2014},
	pages = {1111--1133},
	file = {Agarwal_et_al_2014_A_Reliable_Effective_Terascale_Linear_Learning_System.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/large-scale-learning/Agarwal_et_al_2014_A_Reliable_Effective_Terascale_Linear_Learning_System.pdf:application/pdf}
}

@book{wald_sequential_1973,
	title = {Sequential {Analysis}},
	isbn = {978-0-486-61579-0},
	abstract = {In 1943, while in charge of Columbia University's Statistical Research Group, Abraham Wald devised Sequential Design, an innovative statistical inference system. Because the decision to terminate an experiment is not predetermined, sequential analysis can arrive at a decision much sooner and with substantially fewer observations than equally reliable test procedures based on a predetermined number of observations. The system's immense value was immediately recognized, and its use was restricted to wartime research and procedures. In 1945, it was released to the public and has since revolutionized many aspects of statistical practice. This book is Professor Wald's own description of the system. Part I contains a discussion of the general theory of the sequential probability ratio test, with comparisons to traditional statistical inference systems. Part II discusses applications that illustrate the general theory and raise points of theoretical interest specific to these applications. Part III outlines a possible approach to the problem of sequential multi-valued decisions and estimation. All three sections can be understood by readers with only a background in college algebra and a first course in calculus. Mathematical derivations of somewhat intricate nature appear in the appendix. Sequential Analysis offers statistical researchers a time- and money-saving approach, introduces students to one of the major systems in contemporary use, and presents those already acquainted with the system with valuable background information.},
	language = {en},
	publisher = {Courier Corporation},
	author = {Wald, Abraham},
	year = {1973},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@inproceedings{chen_xgboost:_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2018-04-29},
	booktitle = {Proceedings of the 22Nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	keywords = {large-scale, learning, machine},
	pages = {785--794},
	file = {Chen_Guestrin_2016_XGBoost.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Chen_Guestrin_2016_XGBoost.pdf:application/pdf}
}

@inproceedings{bradley_filterboost:_2007,
	address = {USA},
	series = {{NIPS}'07},
	title = {{FilterBoost}: {Regression} and {Classification} on {Large} {Datasets}},
	isbn = {978-1-60560-352-0},
	shorttitle = {{FilterBoost}},
	abstract = {We study boosting in the filtering setting, where the booster draws examples from an oracle instead of using a fixed training set and so may train efficiently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, \& Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the first proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the filtering setting. Our proofs demonstrate the algorithm's strong theoretical properties for both classification and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overfitting than batch boosters in conditional probability estimation and proves competitive in classification.},
	urldate = {2018-04-29},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Bradley, Joseph K. and Schapire, Robert E.},
	year = {2007},
	pages = {185--192},
	file = {Bradley_Schapire_2007_FilterBoost.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Bradley_Schapire_2007_FilterBoost.pdf:application/pdf}
}

@incollection{ke_lightgbm:_2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	shorttitle = {{LightGBM}},
	urldate = {2018-04-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3146--3154},
	file = {Ke_et_al_2017_LightGBM.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Ke_et_al_2017_LightGBM.pdf:application/pdf;NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/HPYX3H3G/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.html:text/html}
}

@book{owen_monte_2013,
	title = {Monte {Carlo} {Theory}, {Methods} and {Examples}},
	author = {Owen, Art B.},
	year = {2013}
}

@inproceedings{mcsherry_scalability!_2015,
	address = {Kartause Ittingen, Switzerland},
	title = {Scalability! {But} at what {COST}?},
	booktitle = {15th {Workshop} on {Hot} {Topics} in {Operating} {Systems} ({HotOS} {XV})},
	publisher = {USENIX Association},
	author = {McSherry, Frank and Isard, Michael and Murray, Derek G.},
	year = {2015},
	file = {Scalability! But at what COST  USENIX.pdf:/Users/arapat/Documents/Geisel/storage/FNMRSIUH/Scalability! But at what COST  USENIX.pdf:application/pdf;Scalability! But at what COST? | USENIX:/Users/arapat/Documents/Geisel/storage/YLQJS76A/mcsherry.html:text/html}
}

@article{zaharia_apache_2016,
	title = {Apache {Spark}: {A} {Unified} {Engine} for {Big} {Data} {Processing}},
	volume = {59},
	issn = {0001-0782},
	shorttitle = {Apache {Spark}},
	doi = {10.1145/2934664},
	abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
	number = {11},
	urldate = {2018-04-29},
	journal = {Commun. ACM},
	author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
	month = oct,
	year = {2016},
	pages = {56--65},
	file = {Zaharia_et_al_2016_Apache_Spark.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/distributed-learning-in-general/Zaharia_et_al_2016_Apache_Spark.pdf:application/pdf}
}

@incollection{recht_hogwild:_2011,
	title = {Hogwild: {A} {Lock}-{Free} {Approach} to {Parallelizing} {Stochastic} {Gradient} {Descent}},
	shorttitle = {Hogwild},
	urldate = {2018-04-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
	publisher = {Curran Associates, Inc.},
	author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	pages = {693--701},
	file = {NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/ED3YXDWM/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.html:text/html;Recht_et_al_2011_Hogwild.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/optimization/Recht_et_al_2011_Hogwild.pdf:application/pdf}
}

@article{caragea_framework_2004,
	title = {A {Framework} for {Learning} from {Distributed} {Data} {Using} {Sufficient} {Statistics} and {Its} {Application} to {Learning} {Decision} {Trees}},
	volume = {1},
	issn = {1448-5869},
	doi = {10.3233/HIS-2004-11-210},
	abstract = {This paper motivates and precisely formulates the problem of learning from distributed data; describes a general strategy for transforming traditional machine learning algorithms into algorithms for learning from distributed data; demonstrates the ap},
	language = {en},
	number = {1-2},
	urldate = {2018-04-29},
	journal = {International Journal of Hybrid Intelligent Systems},
	author = {Caragea, Doina and Silvescu, Adrian and Honavar, Vasant},
	month = jan,
	year = {2004},
	pages = {80--89},
	file = {Snapshot:/Users/arapat/Documents/Geisel/storage/7Z72LX4S/his010.html:text/html}
}

@inproceedings{viola_rapid_2001,
	title = {Rapid {Object} {Detection} using a {Boosted} {Cascade} of {Simple} {Features}},
	volume = {1},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	author = {Viola, P. and Jones, M.},
	year = {2001},
	keywords = {AdaBoost, background regions, boosted simple feature cascade, classifiers, Detectors, face detection, Face detection, feature extraction, Filters, Focusing, image classification, image processing, image representation, Image representation, integral image, learning (artificial intelligence), machine learning, Machine learning, object detection, Object detection, object specific focus-of-attention mechanism, Pixel, rapid object detection, real-time applications, Robustness, Skin, statistical guarantees, visual object detection},
	pages = {I--511--I--518 vol.1},
	file = {IEEE Xplore Abstract Record:/Users/arapat/Documents/Geisel/storage/7TDQWLWB/990517.html:text/html}
}

@book{schapire_boosting:_2012,
	title = {Boosting: {Foundations} and {Algorithms}},
	isbn = {978-0-262-01718-3},
	shorttitle = {Boosting},
	language = {en},
	publisher = {MIT Press},
	author = {Schapire, Robert E. and Freund, Yoav},
	year = {2012},
	keywords = {Computers / Machine Theory, Computers / Programming / Algorithms}
}

@inproceedings{mason_boosting_1999,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'99},
	title = {Boosting {Algorithms} {As} {Gradient} {Descent}},
	abstract = {We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.},
	urldate = {2018-05-03},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},
	year = {1999},
	pages = {512--518}
}

@article{hoeffding_probability_1963,
	title = {Probability {Inequalities} for {Sums} of {Bounded} {Random} {Variables}},
	volume = {58},
	issn = {0162-1459},
	doi = {10.2307/2282952},
	abstract = {Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for {\textless}tex-math{\textgreater}\${\textbackslash}Pr {\textbackslash}\{ S - ES {\textbackslash}geq nt {\textbackslash}\}\${\textless}/tex-math{\textgreater} depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population.},
	number = {301},
	urldate = {2018-05-03},
	journal = {Journal of the American Statistical Association},
	author = {Hoeffding, Wassily},
	year = {1963},
	pages = {13--30},
	file = {Hoeffding_1963_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/statistics/Hoeffding_1963_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables.pdf:application/pdf;Snapshot:/Users/arapat/Documents/Geisel/storage/WBBPID6J/01621459.1963.html:text/html}
}

@article{valiant_bridging_1990,
	title = {A {Bridging} {Model} for {Parallel} {Computation}},
	volume = {33},
	issn = {0001-0782},
	doi = {10.1145/79173.79181},
	abstract = {The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.},
	number = {8},
	urldate = {2018-05-17},
	journal = {Commun. ACM},
	author = {Valiant, Leslie G.},
	month = aug,
	year = {1990},
	pages = {103--111},
	file = {Valiant_1990_A_Bridging_Model_for_Parallel_Computation.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/synchronous/Valiant_1990_A_Bridging_Model_for_Parallel_Computation.pdf:application/pdf}
}

@inproceedings{liu_asynchronous_2014,
	title = {An {Asynchronous} {Parallel} {Stochastic} {Coordinate} {Descent} {Algorithm}},
	abstract = {We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on...},
	language = {en},
	urldate = {2018-05-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Liu, Ji and Wright, Steve and Re, Christopher and Bittorf, Victor and Sridhar, Srikrishna},
	month = jan,
	year = {2014},
	pages = {469--477},
	file = {Liu_et_al_2014_An_Asynchronous_Parallel_Stochastic_Coordinate_Descent_Algorithm.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/asynchronous/Liu_et_al_2014_An_Asynchronous_Parallel_Stochastic_Coordinate_Descent_Algorithm.pdf:application/pdf;Snapshot:/Users/arapat/Documents/Geisel/storage/XH8VGTNX/liud14.html:text/html}
}

@incollection{lian_asynchronous_2015,
	title = {Asynchronous {Parallel} {Stochastic} {Gradient} for {Nonconvex} {Optimization}},
	urldate = {2018-05-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2737--2745},
	file = {Lian_et_al_2015_Asynchronous_Parallel_Stochastic_Gradient_for_Nonconvex_Optimization.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/asynchronous/Lian_et_al_2015_Asynchronous_Parallel_Stochastic_Gradient_for_Nonconvex_Optimization.pdf:application/pdf;NIPS Snapshort:/Users/arapat/Documents/Geisel/storage/47ZX6T2C/5751-asynchronous-parallel-stochastic-gradient-for-nonconvex-optimization.html:text/html}
}

@article{schapire_improved_1999,
	title = {Improved {Boosting} {Algorithms} {Using} {Confidence}-rated {Predictions}},
	volume = {37},
	issn = {1573-0565},
	doi = {10.1023/A:1007614523901},
	abstract = {We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.},
	language = {en},
	number = {3},
	urldate = {2018-09-27},
	journal = {Machine Learning},
	author = {Schapire, Robert E. and Singer, Yoram},
	month = dec,
	year = {1999},
	keywords = {boosting algorithms, decision trees, multiclass classification, output coding},
	pages = {297--336},
	file = {Schapire_Singer_1999_Improved_Boosting_Algorithms_Using_Confidence-rated_Predictions.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Schapire_Singer_1999_Improved_Boosting_Algorithms_Using_Confidence-rated_Predictions.pdf:application/pdf}
}

@inproceedings{freund_alternating_1999,
	address = {San Francisco, CA, USA},
	series = {{ICML} '99},
	title = {The {Alternating} {Decision} {Tree} {Learning} {Algorithm}},
	isbn = {978-1-55860-612-8},
	urldate = {2018-04-29},
	booktitle = {Proceedings of the {Sixteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Freund, Yoav and Mason, Llew},
	year = {1999},
	pages = {124--133},
	file = {Freund_Mason_1999_The_Alternating_Decision_Tree_Learning_Algorithm.pdf:/Users/arapat/Dropbox/documents/zotero/02. Research/boosting/Freund_Mason_1999_The_Alternating_Decision_Tree_Learning_Algorithm.pdf:application/pdf}
}

@article{friedman_greedy_2001,
	title = {Greedy {Function} {Approximation}: {A} {Gradient} {Boosting} {Machine}},
	volume = {29},
	issn = {0090-5364},
	shorttitle = {Greedy {Function} {Approximation}},
	url = {https://www.jstor.org/stable/2699986},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	number = {5},
	urldate = {2018-09-27},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	year = {2001},
	pages = {1189--1232}
}

@book{hastie_elements_2009,
	address = {New York},
	edition = {2},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	url = {//www.springer.com/us/book/9780387848570},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
	language = {en},
	urldate = {2018-04-29},
	publisher = {Springer-Verlag},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	file = {Snapshot:/Users/arapat/Documents/Geisel/storage/SERV8TF2/9780387848570.html:text/html}
}
@article{friedman_additive_2000,
	title = {Additive logistic regression: a statistical view of boosting ({With} discussion and a rejoinder by the authors)},
	volume = {28},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Additive logistic regression},
	url = {https://projecteuclid.org/euclid.aos/1016218223},
	doi = {10.1214/aos/1016218223},
	abstract = {Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.},
	language = {EN},
	number = {2},
	urldate = {2018-09-28},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2000},
	mrnumber = {MR1790002},
	zmnumber = {1106.62323},
	keywords = {classification, machine learning, nonparametric estimation, stagewise fitting, tree},
	pages = {337--407},
	file = {Friedman_et_al_2000_Additive_logistic_regression.pdf:/Users/arapat/Dropbox/documents/zotero/Friedman_et_al_2000_Additive_logistic_regression.pdf:application/pdf;Snapshot:/Users/arapat/Documents/Geisel/storage/NI687TZR/1016218223.html:text/html}
}
@article{schapire_boosting_1998,
	title = {Boosting the margin: a new explanation for the effectiveness of voting methods},
	volume = {26},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Boosting the margin},
	url = {https://projecteuclid.org/euclid.aos/1024691352},
	doi = {10.1214/aos/1024691352},
	abstract = {One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik’s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.},
	language = {en},
	number = {5},
	urldate = {2018-09-28},
	journal = {The Annals of Statistics},
	author = {Schapire, Robert E. and Freund, Yoav and Bartlett, Peter and Lee, Wee Sun},
	month = oct,
	year = {1998},
	mrnumber = {MR1673273},
	zmnumber = {0929.62069},
	keywords = {bagging, boosting, decision trees, Ensemble methods, error-correcting, Markov chain, Monte Carlo, neural networks, output coding},
	pages = {1651--1686},
	file = {Schapire_et_al_1998_Boosting_the_margin.pdf:/Users/arapat/Dropbox/documents/zotero/Schapire_et_al_1998_Boosting_the_margin.pdf:application/pdf;Snapshot:/Users/arapat/Documents/Geisel/storage/S9TRKNIH/1024691352.html:text/html}
}