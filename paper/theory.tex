\section{Theory}\label{sec:theory}
We start with a brief description of the confidence-rated boosting
algorithm (Algorithm 9.1 on the page 274 of \cite{schapire_boosting:_2012}).

Let $\vx \in X$ be the feature vectors and let the output be $y \in
Y= \{-1,+1\}$. The target is defined as a joint distribution $\Dist$ over
$X \times Y$, our goal is to find a classifier $c: X \to Y$ with small
error:
$$\err_{\Dist}(c) \doteq \Prob{(\vx,y)\sim \Dist}{c(\vx) \neq y}$$

We are given a set $\cH$ of base classifiers $h:X \to [-1,+1]$. The
Score function generated by AdaBoost is a weighted sum of $T$ rules from
$\cH$
\[
S_T(\vx) = \left( \sum_{t=1}^T \alpha_t h_t(\vx) \right)
\]

The strong classifier is the sign of the score function: $H_T =
\sign(S_T)$.  

AdaBoost is a gradient descent algorithm. It operates by iteratively
decreasing the value of an exponential potential function\footnote{Other
  potential functions have been studied. In this work we restrict
  ourselves to the original exponential potential function.}  We start
with the {\em true} potential, i.e. the potential in the limit where
the size of the training set increases to infinity. We will then
consider the realistic case, where the size of the training set is
finite.  The {\em true} potential of the score function $S_t$ is
\[
\Phi(S_t) = \Expect{(\vx,y) \sim \Dist}{e^{-S_t(\vx)y}}
\]
Consider adding a single base rule $h_t$ to the score function
$S_{t-1}$:
$S_t=S_{t-1}+\alpha_t h_t$ and taking the partial derivative of the potential with respect
to $\alpha_t$ we get:
\begin{equation} \label{eqn:weights}
\left. \frac{\partial}{\partial \alpha_t}\right|_{\alpha_t=0} \Phi(S_{t-1}+\alpha_t h)
= \Expect{(\vx,y) \sim \Dist_{t-1}}{h(\vx) y}
\end{equation}
Where
\begin{equation} ~\label{eqn:Dt}
\Dist_{t-1} = \frac{\Dist}{Z_{t-1}} \exp\left( -S_{t-1}(\vx)y \right)
\end{equation}
and $Z_{t-1}$ is a normalization factor that makes $\Dist_{t-1}$ a
distribution.

AdaBoost performs coordinate-wise gradient descent where each
coordinate corresponds to one base rule. Using
equation~\ref{eqn:weights} we can express the gradient with respect to
base rule $h$ as a correlation, which we also call the {\em true edge}: 
\begin{equation} \label{eqn:true-edge}
\edge_{t}(h) \doteq \corr_{\Dist_{t-1}}(h) \doteq \Expect{(\vx,y) \sim \Dist_{t-1}}{h(\vx) y}
\end{equation}

The goal of a single boosting step is to find a base rule with
significant edge.~\footnote{Some times the goal is stated as
  finding the rule with the {\em maximal} edge.  Here we require only
  that $h_t$ has a significant edge.}

Up to this point, our discussion regarded the true expected value or
an infinitely large training set. Real training sets are finite, 
therefore \Sparrow\ has to {\em
  estimate} the edge of $h$. We use the standard unbiased estimate
\begin{equation} \label{eqn:emp-edge}
\edgeEmp_{t}(h) \doteq \corrEmp_{\Dist_{t-1}}(h)
\doteq 
\sum_{i=1}^n \frac{w_i}{Z_{t-1}} h(\vx_i) y_i
\end{equation}
where  $w_i = e^{-S_{t-1}(\vx_i)}$ and $Z_{t-1} = \sum_{i=1}^n w_i$

The novelty of \Sparrow\ is in the way it uses samples of the training
data to to identify rules whose true edge is significant.
Several statistical techniques are used to minimize the number of
examples needed to compute the estimates.

\subsection{Effective Sample Size}
\label{sec:effectiveSampleSize}
Equation~\ref{eqn:emp-edge} defines $\edgeEmp(h)$, which is an
unbiased estimate of $\edge(h)$. How accurate is this estimate? A
standard quantifier is the variance of the estimator:
\begin{equation} \label{eqn:variance}
 \mbox{Var}(\edgeEmp) = \frac{\sum_{i=1}^n w_i^2}{\left(\sum_{i=1}^n w_i\right)^2}
\end{equation}
If all of the weights are equal $\mbox{Var}(\edgeEmp) = 1/n$ which
corresponds to a standard deviation of $1/\sqrt{n}$ which is the
expected relation between the sample size and the error.

If the weights are not equal then the variance is larger and the
estimate is less accurate. We define the {\em effective sample size}
$\neff$ to be
\begin{equation} \label{eqn:neff}
  \neff \doteq \frac{\left(\sum_{i=1}^n w_i\right)^2}{\sum_{i=1}^n w_i^2}
\end{equation}
So that $\mbox{Var}(\edgeEmp) = 1/\neff$.

To see that the name ``effective sample size'' makes sense, consider
$n$ weights where $w_1=\cdots,w_k=1/k$ and
$w_{k+1}=\cdots=w_{n}=0$. It is easy to verify that in this case
$\neff=k$ which agrees with our intuition that examples with zero
weight have no effect on the estimate.

\Sparrow\ samples examples from disk to memory using {\em weighted
  sampling}. This is a sequential algorithm that reads from disk one
example $(\vx,y)$ at a time, calculates the weight for that example
$w_i$ and the flips a biased coin to accept the example with
probability $w_i/M$, where $M\geq \max w_i$. Accepted examples are
stored in main memory with initial weight of $1$.

LightGBM also uses sampling, but its sampling method is biased.

Suppose $n$ initial memory-resident training examples consist of 99\%
negative and 1\% positive examples. In that case the first base rule
will be ``predict negative everywhere''. After reweighting the total
weights of the positive and negative examples will be equal. This in
turn means that the the effective size of about $\neff \approx
n/25$. In other words, the variance of estimates of the edge has
increased by a factor of 25 relative to the initial uniform weights.

When \Sparrow\ detects that $\neff$ is small. It clears the memory and
samples a new set from disk. However, as the sampling uses the weights
as acceptance probabilities, half of the training data will be
positive and half of them will be negative and $\neff$ is back to $n$.

This process continues as long as AdaBoost is making progress and the
weights are becoming increasingly skewed. When the skew is large,
$\neff$ is small and \Sparrow\ resamples a new sample with uniform
weights.

The net effect is that even though the memory-resident samples are
small, they contain the important examples, whose weight is large.


\subsection{Sequential analysis}

\Sparrow\ achieves Disk-to-Memory efficiency by using weighted
resampling that is triggered when $\neff$ is too small.

\Sparrow\ achieve Memory-to-CPU efficiency by reading from memory the
minimal number of examples that are necessary to establish that a
particular weak rule has a significant edge. This is done using
Sequential Analysis and Early Stopping.

Sequential analysis (SA) was introduced by
Wald~\cite{wald_sequential_1973} in the 1940s.  Here we give a short
illustration. Suppose we want to estimate the expected loss of a
model. In the standard large deviation analysis we assume that the
loss is bounded in some range, say $[-M,+M]$ and that the size of the
training set is $n$. This implies that the standard deviation of the
training loss is at most $M/\sqrt{n}$. In order to make this standard
deviation be smaller than some $\epsilon>0$ we need that $n >
(M/\epsilon)^2$. While this analysis is optimal in the worst case, it
can be improved if we have additional information about the standard
deviation. We can glean such information from the observed losses by
using the following sequential analysis method. Instead of choosing
$n$ ahead of time, the algorithm computes the loss of one example at a
time. A {\em stopping rule} is used to decide whether, conditioned on
the sequence of losses seen so far, there is very small probability
that the difference between the average loss and the true loss is
larger than $\epsilon>0$. The result is that when the standard
deviation is significantly smaller than $M$ the number of examples
that need to be used in the estimate is much smaller than
$n=(M/\epsilon)^2$.

\subsection{\Sparrow's stopping rule} \label{sec:balsubramani}

Using stopping rules for AdaBoost was proposed in
~\cite{domingo_scaling_2000, bradley_filterboost:_2007}. Each use a
different stopping rule. Unlike those works, we are interested in a
rule that will take into account the fact that examples have different
weights which impacts the the variance of the estimator. We want a
stopping rule that is sensitive to $\neff$.

Note that the choice of stopping rule has a direct impact on the
performance of the algorithm. In other words, we want to use a
stopping rule that is as tight as possible, including constants.

We use a stopping rule proposed in~\cite{balsubramani_sharp_2014}
for which they prove the following:

\begin{theorem}[based on \cite{balsubramani_sharp_2014} Theorem 4] \label{thm:balsubramani}
  Let $M_t$ be a martingale $M_t = \sum_i^t X_i$,
  and suppose there are constants $\{c_k\}_{k \geq 1}$ such that
  for all $i \geq 1$, $|X_i| \leq c_i$ w.p.\ 1.
  For $\forall \sigma > 0$, with probability at least $1 - \sigma$ we have
  \[
  \forall t: |M_t| \leq C \sqrt{
    \left( \sum_{i=1}^t c_i^2 \right)
    \left( \log \log \left( \frac{ \sum_{i=1}^t c_i^2 }{ |M_t| }\right) +
    \log \frac{1}{\sigma} \right)
  },
  \]
  where $C$ is a universal constant.
\end{theorem}

As we care about constants, we did a series of experiments to find the
best constant $C$ which we use in \Sparrow.
