\section{Theory}
The analysis given in this section applies to the original boosting
algorithm~\cite{orig, confidence-rated, adtrees} and to \Sparrow\. It
does not apply to XGboost~\cite{} or to LightGBM~\cite{} which are are
both implementations of Gradient-Boosted-Trees~\cite{}, which does not
lend itself to this type of analysis.

The inner loop of the original Adaboost algorithm is a search for the
weak rule with the smallest weighted error. This rule is found by
computing the weighted error of each weak rule using the whole
training set. When the training set is large, this scan can take a
long time.

\Sparrow\ reduces the required time by sampling subsets of the data into
memory and by reading just enough data to find a good (rather than the
best) weak rule. These techniques are based on Sequential Analysis and
on selective sampling, which we now describe.

\subsection*{Sequential analysis}
Sequential analysis (SA) was introduced by
Wald\cite{wald_sequential_1973} in the 1940s.  Here we give a short
illustration. Suppose we want to estimate the expected loss of a
model. In the standard large deviation analysis we assume that the
loss is bounded in some range, say $[-M,+M]$ and that the size of the
training set is $n$. This imples that the standard deviation of the
training loss is at most $M/\sqrt{n}$. In order that this standard
deviaton is smaller than some $\epsilon>0$ we need that
$n > (M/\epsilon)^2$. While this analysis is optimal in the worst case, it
can be improved if we have additional information about the standard
deviation. We can glean such information from the observed losses by
using the following sequential analysis method. Instead of choosing
$n$ ahead of time, the algorithm computes the loss of one example at a
time. A {\em stopping rule} is used to decide whether, conditioned on
the sequence of losses seen so far, there is very small probability
that the difference between the average loss and the true loss is
larger than $\epsilon>0$. The result is that when the standard
deviation is significantly smaller than $M$ the number of examples
that need to be used in the estimate is much smaller than
$n=(M/\epsilon)^2$.

\subsection{Sequential Analysis for Boosting}\label{sec:methods:early-stop}

A small but important observation is that Adaboost does not require
finding the weak rule with the {\bf smallest} weighted error at each
iteration. Rather, it is enough to find a rule for which we are sure
that it has a significant (but not necessarily maximal) advantage over
random guessing.

(define $\Dist$, $\edge$ and $\edgeEmp$)
More precisely, we want to know that, with high
probability over the choice of $\training \sim \Dist^n$ the rule $h$
has a significant {\em true} edge $\edge(h)$.

Domingo and Watanabe~\cite{domingo_scaling_2000} and Bradley and
Schapire~\cite{bradley_filterboost:_2007} proposed using early
stopping to take advantage of such situations. The idea is simple:
instead of scanning through all of the training examples when
searching for the next weak rule, a {\em stopping rule} is checked for
each $h \in \cH$ after each training example, and if this stopping
rule ``fires'' then the scan is terminated and the $h$ that caused the
rule to fire is added to the strong rule.

{\bf note that if a few rules are good and most are bad, this gives a
  significant reduction in sample size.}

\cite{bradley_filterboost:_2007} and~\cite{domingo_scaling_2000}
define use stopping rules that apply to equally weighted sampled
data. For reasons that will be explained in the next section, we use a different
 We use a different stopping rule that will be explained in
 Section~\ref{sec:balsubramani}.


\paragraph{Effective Sample Size}
\label{sec:effectiveSampleSize}
Equation~\ref{eqn:gamma_emp} defines $\edgeEmp(h)$, which is an
estimate of $\edge(h)$. How accurate is this estimate? Our initial
gut reaction is that if $\training$ contains $n$ examples the error should be
about $1/\sqrt{n}$. However, when the examples are weighted this is
clearly wrong. Suppose, for example that $k$ out of the $n$ examples
have weight one and the rest have weight zero. Obviously in this case
we cannot hope for an error smaller than $1/\sqrt{k}$.

A more quantitative analysis follows. Suppose that the weights of the
examples in the training set $\training=\{ (x_1, y_1), \ldots, (x_n,
y_n) \}$ are $w_1=w(x_1,y_1),\ldots,w_n=w(x_n,y_n)$. Thinking of
finding a good weak rule in terms of hypothesis testing, the null
hypothesis is that the weak rule $h$ has no edge. Finding a rule that
is significantly better than random corresponds to rejecting the
hypothesis that $\edge(h)=0$.  Assuming the null hypothesis, $y_i
h(x_i)$ is $+1$ with probability 1/2 and $-1$ with probability
$1/2$. From central limit theorem and assuming $n$ is larger than
$100$, we get that the null distribution for $\edgeEmp(h)=\sum_{i=1}^n
w_i y_i h(x_i)$ is normal with zero mean and standard deviation
$\sum_{i=1}^n w_i^2$. The statistical test one would use in this case
is the $\Ztest$-test for
\begin{equation} \label{eqn:Ztest}
\Ztest = \frac{\edgeEmp(h)}{\sqrt{\sum_{i=1}^n w_i^2}}
= \frac{\sum_{i=1}^n w_i y_i h(x_i)}{\sqrt{\sum_{i=1}^n w_i^2}}
\end{equation}
As should be expected, the value of $\Ztest$ remains the same whether
or not $\sum_{i=1}^n w_i=1$. Based on Equation~\ref{eqn:Ztest} we
define the {\em effective number of examples} corresponding to the
un-normalized weights $w_1,\ldots,w_n$ as:
\begin{equation} \label{eqn:neff}
  \neff \doteq \frac{\left(\sum_{i=1}^n w_i\right)^2}{\sum_{i=1}^n w_i^2}
\end{equation}
Owen~\cite{owen_monte_2013} used a different line of
argument to arrived at a similar measure of
the effective samples size for a weighted sample.

The quantity $\neff$ plays a similar role in large deviation bounds
such as the Hoeffding bound~\cite{hoeffding_probability_1963} (details ommitted).
It also plays
a central role in Theorem~\ref{thm:balsubramani} and thus in the
stopping rule that we use.

To understand the important role that $\neff$ plays in our algorithm,
supppose the training set is of size $n$ and that only $m \ll n$
examples can fit in memory. Our approach is to start by placing a
random subset of size $m$ into memory and then run multiple
boosting iterations using this subset. As the strong rule improves,
$\neff$ decreases and as a result the stopping rule based on
Theorem~\ref{thm:balsubramani} requires increasingly more examples
before it is triggered. When $\neff/m$ crosses a pre-specified
threshold the algorithm flushes out the training examples currently in
memory and samples a new set of $m$ examples using acceptance
probability proportional to their weights. The new examples have
uniform weights and therefor after sampling $\neff=m$.

Intuitively, weighted sampling utilizes the computer's memory better
than uniform sampling because it places in memory more difficult
examples and fewer easy examples. The result is better estimates of
the edges of specialist\footnote{Specialist weak rules and their
  advantages are described in Section~{sec:Algorithm}} weak rules that
make predictions on high-weight difficult examples.


Another concern is the fraction of the examples that are selected. In
the method described here the expected fraction is $(\frac{1}{n}
\sum_{i=1}^n w_i)/(\max_i w_i)$.

\subsection{\Sparrow's stopping rule} \label{sec:balsubramani}
 stopping rule proposed
in~\cite{balsubramani_sharp_2014} for which they prove the following

\begin{theorem}[based on \cite{balsubramani_sharp_2014} Theorem 4] \label{thm:balsubramani}
  Let $M_t$ be a martingale $M_t = \sum_i^t X_i$,
  and suppose there are constants $\{c_k\}_{k \geq 1}$ such that
  for all $i \geq 1$, $|X_i| \leq c_i$ w.p.\ 1.
  For $\forall \sigma > 0$, with probability at least $1 - \sigma$ we have
  \[
  \forall t: |M_t| \leq C \sqrt{
    \left( \sum_{i=1}^t c_i^2 \right)
    \left( \log \log \left( \frac{ \sum_{i=1}^t c_i^2 }{ |M_t| }\right) +
    \log \frac{1}{\sigma} \right)
  },
  \]
  where $C$ is a universal constant.
\end{theorem}
